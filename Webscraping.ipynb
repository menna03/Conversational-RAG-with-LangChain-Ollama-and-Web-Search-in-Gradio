{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e72d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Base URL for LangChain documentation\n",
    "BASE_URL = \"https://python.langchain.com/docs/\"\n",
    "DOCS_DIR = \"langchain_docs\"\n",
    "VISITED_URLS = set()\n",
    "MAX_DEPTH = 3  # Limit crawling depth to avoid going too far off track\n",
    "REQUEST_DELAY = 0.5  # Seconds to wait between requests\n",
    "\n",
    "# Initialize html2text converter\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = False  # Keep links as Markdown links\n",
    "h.ignore_images = True\n",
    "h.body_width = 0  # Don't wrap lines\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Sanitizes a string to be a valid filename.\"\"\"\n",
    "    name = name.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    name = \"\".join([c if c.isalnum() or c in ('.', '_', '-') else '_' for c in name])\n",
    "    name = name.strip('_.-')\n",
    "    return name if name else \"index\"\n",
    "\n",
    "def get_and_save_page(url, current_depth):\n",
    "    if url in VISITED_URLS or current_depth > MAX_DEPTH:\n",
    "        return\n",
    "\n",
    "    # Only process URLs within the desired docs path\n",
    "    if not url.startswith(BASE_URL):\n",
    "        return\n",
    "\n",
    "    VISITED_URLS.add(url)\n",
    "    logging.info(f\"Processing (Depth {current_depth}): {url}\")\n",
    "\n",
    "    try:\n",
    "        # Make request with a delay\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        response = requests.get(url, timeout=20, headers={'User-Agent': 'LangChainDocsScraper/1.0'})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Locate the main content area (article or main tag)\n",
    "        main_content_area = soup.find('article') or soup.find('main') or soup.body\n",
    "        \n",
    "        if not main_content_area:\n",
    "            logging.error(f\"No content area found for {url}\")\n",
    "            return\n",
    "\n",
    "        # Convert HTML of main content to Markdown\n",
    "        markdown_content = h.handle(str(main_content_area))\n",
    "\n",
    "        if not markdown_content.strip():\n",
    "            logging.warning(f\"No text content extracted from {url}\")\n",
    "            return\n",
    "\n",
    "        # Create a sanitized filename\n",
    "        parsed_url = urlparse(url)\n",
    "        path_parts = [part for part in parsed_url.path.split('/') if part]\n",
    "        \n",
    "        # Extract relevant path for the filename\n",
    "        filename_parts = []\n",
    "        try:\n",
    "            docs_index = path_parts.index('docs')\n",
    "            filename_parts = path_parts[docs_index+1:]\n",
    "        except ValueError:\n",
    "            filename_parts = path_parts\n",
    "\n",
    "        filename_base = sanitize_filename(\"_\".join(filename_parts)) or \"index\"\n",
    "\n",
    "        # Ensure .md extension\n",
    "        filename = f\"{filename_base}.md\" if not filename_base.endswith(\".md\") else filename_base\n",
    "        filepath = os.path.join(DOCS_DIR, filename)\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "        # Save the Markdown content\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Source URL: {url}\\n\\n\")\n",
    "            f.write(markdown_content)\n",
    "        logging.info(f\"Saved: {filepath}\")\n",
    "\n",
    "        # Process other links on the page\n",
    "        for link in main_content_area.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            next_url = urljoin(url, href)  # Handle relative URLs\n",
    "\n",
    "            # Skip anchors and non-HTTP(S) links\n",
    "            if urlparse(next_url).fragment or not next_url.startswith(('http://', 'https://')):\n",
    "                continue\n",
    "\n",
    "            get_and_save_page(next_url, current_depth + 1)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {url}: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOCS_DIR):\n",
    "        os.makedirs(DOCS_DIR)\n",
    "\n",
    "    # Start with the main docs page and key sections\n",
    "    initial_urls = [\n",
    "        \"https://python.langchain.com/docs/get_started\",\n",
    "        \"https://python.langchain.com/docs/modules\",\n",
    "        \"https://python.langchain.com/docs/use_cases\",\n",
    "        \"https://python.langchain.com/docs/integrations\",\n",
    "        \"https://python.langchain.com/docs/guides\",\n",
    "        \"https://python.langchain.com/docs/concepts\",\n",
    "        \"https://python.langchain.com/docs/expression_language\",\n",
    "    ]\n",
    "\n",
    "    # Crawl the documentation starting from the initial URLs\n",
    "    for start_url in initial_urls:\n",
    "        get_and_save_page(start_url, 0)\n",
    "\n",
    "    logging.info(f\"Documentation download process complete. Check the '{DOCS_DIR}' directory.\")\n",
    "    logging.info(f\"Total unique URLs visited: {len(VISITED_URLS)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
